# -*- coding: utf-8 -*-
"""RDD Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ns2T4ezlizuqrAVygPjTm8_vKMH2HF8L
"""

import csv
# Set up SparkContext
from pyspark import SparkConf, SparkContext
conf = SparkConf().setAppName("AverageFriendsByAge").setMaster("local[*]")
sc = SparkContext(conf=conf)

# Path to file, change it
file_path = "/content/fakefriends.csv"

def parse_line(line):
    fields = line.split(",")
    age = int(fields[2])
    num_friends = int(fields[3])
    return (age, num_friends)

lines = sc.textFile(file_path)
age_friends_rdd = lines.map(parse_line)

# Transform, Reduce
totals_by_age = age_friends_rdd.mapValues(lambda x: (x, 1))

sum_count_by_age = totals_by_age.reduceByKey(
    lambda x, y: (x[0] + y[0], x[1] + y[1]))

# Average: (age, (sum, count)) â†’ (age, avg)
avg_friends_by_age = sum_count_by_age.mapValues(lambda x: round(x[0] / x[1], 2))

results = avg_friends_by_age.collect()
for age, avg_friends in sorted(results):
    print(f"({age}, {avg_friends})")

# Export to CSV
output_path = "/content/average_friends_by_age.csv"

with open(output_path, "w", newline="") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["Age", "AverageFriends"])  # Header
    writer.writerows(sorted(results))

print(f"CSV file saved to: {output_path}")

# Stop SparkContext
sc.stop()